%!TEX root=./thesis.tex

\begin{algorithm} % [tbp]
\caption{Chopped symbolic execution (simplified).\label{fig:chopped-symbexe-recover}\label{fig:chopped-symbexe} }
\begin{algorithmic}[1]
  \Function{cse}{$\instate$, $\skipFunctions$}
  \State $\worklist \gets \worklist \cup \{ \instate\}$  \label{alg:seed-worklist}
  \While{$\worklist \neq \emptyset$}                     \label{alg:iterate-worklist}
    \State $\workstate \gets  \Call{selectUnsuspended}{\worklist}$ \label{alg:pop-worklist}
    \State $\inst \gets  \Call{instruction}{\workstate}$ \label{alg:get-switch-inst}

    \Switch{$\inst$} \label{alg:switch}
    \Case{Call} \label{alg:casecall-begin}
      \Call{handleCall}{$\workstate, \inst$}
    \EndCase \label{alg:casecall-end}
    \Case{Load} \label{alg:caseload-begin}
      \Call{handleLoad}{$\workstate, \inst$}
    \EndCase \label{alg:caseload-end}
    \Case{Branch} \label{alg:casebranch-begin}
      \Call{handleBranch}{$\workstate, \inst$}
    \EndCase \label{alg:casebranch-end}
    \Case{Store} \label{alg:casestore-begin}
      \Call{handleStore}{$\workstate, \inst$}
    \EndCase \label{alg:casestore-end}
    \Case{Exit} \label{alg:caseexit-begin}
      \Call{handleExit}{$\workstate, \inst$}
    \EndCase \label{alg:caseexit-end}
    \EndSwitch
  \EndWhile
  \EndFunction
\end{algorithmic}
\end{algorithm}


\section{Main Algorithm}\label{section:main-algorithm}

\Cref{fig:chopped-symbexe} presents the key steps in chopped
symbolic execution, which we gradually explain. The algorithm operates
on a simple imperative C-like heap-manipulating language with
assignments, assertions, conditional jumps, dynamic memory allocation
and reclamation, and function calls with call-by-value parameter
passing.
Note that in practice, our algorithm operates on LLVM bitcode~\cite{llvm}.
The function parameters can be of any type: primitives, structs, poitner, \etc
Without loss of generality, we assume that functions do not have a return value.
A function with a return value can always be rewritten with an additional parameter 
that points to the memory location of the return value.

To simplify the explanation, we now
assume that we may skip at most one function invocation at every
explored path, and discuss the general case in
\S\ref{Se:SkipMultipleFuncs}. For the same reason, we also assume that
the program does not dynamically allocate memory, and discuss the
handling of $\code{malloc}$ and $\code{free}$ in \S\ref{Se:Malloc}.

Chopped symbolic execution begins by invoking procedure $\textsc{cse}$
with an \emph{initial symbolic state} ($\instate$) and a set
containing the names of the functions that the user wishes to skip
($\skipFunctions$). We expect a symbolic state $\workstate$ to encode,
among other properties, the next instruction to be executed, the activation record stack,
and a symbolic description of the program \emph{address space}. For example, the
chopped symbolic execution described in \Cref{chapter:overview} begins
with $\instate$ in which the stack contains only the activation record
of $\code{main}$, with the program counter at
line~\ref{line:main_enter}, an empty address space, and
$\skipFunctions = \{ \code{f} \}$.

The pool of execution states is mainted using the $\worklist$,
which is initialized at the beginning with $\instate$ (\cref{alg:seed-worklist}).
Then, a standard worklist-based algorithm starts executing until either the
worklist is empty (\cref{alg:iterate-worklist}), or the algorithm
exhausts the time budget (elided). As usual, the algorithm pops the
symbolic state $\workstate$ to explore out of the worklist
(\cref{alg:pop-worklist}).
Unconventionally, however, we assume that
the selected state is not suspended, since a suspended state is ignored by the execution engine
until it's corresponding recovery state terminates
and the value of the depended load is recovered (see \S\ref{chapter:overview}).
At this point, we defer the discussion regarding the search heuristics to \S\ref{Se:Search},
and assume, for now, that any unsuspended state can be returned.
The next step of the algorithm depends on the instruction type (\cref{alg:switch}).

\subsection{\text{Handling \textit{Call} Instructions}}
A $\instruct{Call}$ instruction is handled at lines~\ref{alg:casecall-begin}--\ref{alg:casecall-end},
as illustrated by step \step{1} in \Cref{fig:simple} (see \S\ref{chapter:overview}).
First, the algorithm determines the name $f$ of the invoked function
(\cref{alg:call-find-target}). Then, if $f$ is one of the skipped
functions, the algorithm creates a snapshot of the current state
$\workstate$ and advances the program counter in $\workstate$
without executing the skipped function (\cref{alg:take-snapshot}).
The newly created $\snapshot$ state is added to the end of its list 
of \emph{skipped invocations} (\cref{alg:record-snapshot}).
A skipped invocation is represented
as a triple $(f,\snapshot)$ composed of the name of a skipped
function $f$ and a $snapshot$ of the symbolic state at the time $f$ was skipped.
If at some point we will need to recover some of the side effects
of the skipped function, the recovery state will be created
by cloning the $\snapshot$ state.

Conversely, if $f$ is not be skipped, the algorithm handles its
invocation as usual in symbolic execution.  For brevity, we elide the
standard handling of commands by symbolic execution.

\subsection{\text{Handling \textit{Load} Instructions}}
A $\instruct{Load}$ instruction is handled
at lines {(lines~\ref{alg:caseload-begin}--\ref{alg:caseload-end})}.
The algorithm calls $\Call{mayMod}{\workstate,\workstate.\skipped,\addr}$,
as shown in \Cref{fig:aux-func-may-mod}
to determine whether the address from
which a value is read ($\addr$) might have been modified by one of the
skipped functions on the path followed by the current state $\workstate$.

The auxiliary function $\textsc{mayMod}$
receives as parameters a symbolic state $\workstate$, a list of
skipped invocations $\funclist$, and an address $\addr$ which is the
target of a $\instruct{Load}$ instruction, and determines whether one
of the skipped function in $\funclist$ may store a value in $\addr$.
As was described in \Cref{section:static-analysis},
a \textit{load} instruction is associated with the allocation sites to
which it may point and which may be modified by a skipped function.
Using the fact that we are running in the context of symbolic execution,
we can determine the memory object which is associated with $\addr$ and more importantly,
determine the exact allocaion site of $\addr$.

The ability to determine the exact allocation site helps us to avoid redundant recoveries.
In \Cref{fig:dependent-load}, $\code{strlen}$ is invoked twice
with two different objects: $\code{a}$ and $\code{b}$.
Since our pointer analysis is context-insensitive,
the points-to set in $\code{stlen}$ contains the allocation sites of both $\code{a}$ and $\code{b}$.
Thus, from the point of view of pointer analysis,
the first invocation of $\code{strlen}$ at line \ref{line:load-a}
may depend on the side effects of $\code{modify}$
(which is not precise, since only the second invocation of $\code{strlen}$ is affected by $\code{modify}$).
However, when symbolically executing the load at line \ref{line:load-a},
we know that the exact allocation site is the variable $\code{a}$
which is not included in the set of objects which may be modified by $\code{modify}$.
As a result, we avoid a redundant recovery of the $\code{modify}$ function.

\begin{figure}
  \centering
    \lstinputlisting[linewidth=.4\textwidth]{code/dependent-load.c}
    \label{fig:dependent-load}
  \caption{Avoiding recovery using allocation site information}
  \label{fig:simpe-dependent-load}
\end{figure}

If $\textsc{mayMod}$ returns true,
then the algorithm calls \textsc{createRecoveryState} in order to create a recovery state.
Otherwise, the $\instruct{Load}$ instruction is handled as usual in symbolic execution (\cref{alg:load-normal}).

Procedure \textsc{createRecoveryState} is shown in
\ref{fig:aux-func-recS}. The function handles $\instruct{Load}$
instructions as illustrated by step \step{2}{} in \Cref{fig:simple}
(see \S\ref{chapter:overview}):
It iterates over the list of skipped
functions (line~\ref{alg:recover-foreach}), and uses
$\Call{mayMod}{\workstate, f, \addr}$ to determine which of the skipped
functions $f$ might have written to the address $\addr$. Once it finds
such a function (\cref{alg:recover-if-found}), the current state
becomes a dependent state (\cref{alg:recover-gen-depS}) and it is
immediately suspended (\cref{alg:recover-suspend}). 
The procedure then creates a \emph{recovery} state $\recoveryState$ by
forking $\snapshot$.
The \textit{guiding constraints}, \ie  the path conditions accumulated in $\workstate$
since the snapshot state was created (\cref{alg:recover-gen-recS,alg:recover-set-is-recS}),
are added to the path conditions of the newly created recovery state.
The algorithm then invokes a static program slicer to remove from the
skipped function $f$ instructions which cannot affect the memory location which corresponds to
the dependent load (\cref{alg:recover-slice}).
At lines~\ref{alg:set-dependent-state} and \ref{alg:set-load-addr},
$\workstate$ is set to be the dependent state of $\recoveryState$,
and the address $\addr$ is set to be the load address of $\recoveryState$.
The recovery state needs to know the address of the dependent load,
so it would be able to update it's dependent state when this address is written.
In addition, once the recovery state terminates (normal return, exit, error),
it's dependent state must be treated appropriately.
Finaly, the recovery state $\recoveryState$ is pushed to the worklist (\cref{alg:push-worklist-recovery}).

\subsection{\text{Handling \textit{Branch} Instructions}}
A $\instruct{Branch}$ instruction is handled
at lines~\ref{alg:casebranch-begin}--\ref{alg:casebranch-end}.
The algorithm checks whether the current state $\workstate$ is a recovery state.
If so, then the $\instruct{Branch}$ instruction is
handled as illustrated by steps \step{4}{} and \step{5}{} in
\Cref{fig:simple} (see \S\ref{chapter:overview}).
It first gets from $\workstate$ the (suspended) dependent state $\dependentState$,
which spawned $\workstate$ as a recovery state (\cref{alg:get-dependent}).
It then determines the branch condition $\varphi$ (\cref{alg:branch-get-cond}).
If $\varphi$ is feasible in $\workstate$,
then it is added to it's path condition as well as to the path conditions
of it's dependent state $\dependentState$ (lines~\ref{alg:add-pc-true-recS}--\ref{alg:add-pc-true-depS}).
If the negation of the branch condition, $\neg \varphi$, is feasible in $\workstate$,
then the (current) recovery state $\workstate$ and it's dependent state $\dependentState$ are forked.
Their path conditions are updated with $\neg \varphi$ (lines~\ref{alg:add-pc-false-recS}--\ref{alg:add-pc-false-depS}),
and both are pushed to the worklist (line~\ref{alg:push-both}).

Since the \textit{guiding constraints} are added to a recovery state at the moment of creation,
a branch which is feasible in a recovery state is guaranteed to feasible in it's dependent state as well.
This means, that whenever we have a fork in a recovery state,
it is enough to check satisfiability only in the recovery state.

If the current state $\workstate$ is not a recovery
state, then the $\instruct{Branch}$ instruction is handled as usual in
symbolic execution (\cref{alg:branch-normal}).

\subsection{\text{Handling \textit{Store} Instructions}}
A $\instruct{Store}$ instruction is handled
at lines~\ref{alg:casestore-begin}--\ref{alg:casestore-end}.
The algorithm executes the $\instruct{Store}$ instruction on the
current state in two steps.
First, the symbolic state $\workstate$ is updated as usual in symbolic execution (\cref{alg:store-normal}).
Then, if $\workstate$ is a recovery state, then the algorithm
updates the dependent state with the same written value as in the recovery state
(as illustrated by step \step{6}{} in \Cref{fig:simple}, see \S\ref{chapter:overview}).
If $\workstate$ is not a recovery state,
then it updates it's set of overwritten addresses to record
that a value was stored in $\addr$ after the skip,
and thus any value that will be written in a skipped function
is no longer relevant (\cref{alg:record-overwrite}). 
Note that at this point the dependent state is not resumed,
even if it was updated with a written value.

\subsection{\text{Handling \textit{Exit} Instructions}}
A $\instruct{Exit}$ instruction is handled
at lines~\ref{alg:caseexit-begin}--\ref{alg:caseexit-end}.
The algorithm checks whether the current state $\workstate$ is a
recovery state. If $\workstate$ is a recovery state \emph{and} the
$\instruct{Exit}$ instruction is invoked inside the skipped function,
then the recovery is terminated and the instruction is handled as
illustrated by step \step{7}{} in \Cref{fig:simple} (see
\S\ref{chapter:overview}): Specifically, the recovery state itself if
discarded (\cref{alg:exit-terminate-recovery}) and the dependent state
is resumed \cref{alg:exit-resume}. Otherwise, the $\instruct{Exit}$
instruction is handled as usual in symbolic execution
(\cref{alg:exit-normal}).

\input{cse-alg}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
